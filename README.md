# Deokso_el_pred

지역 지하수 관리 DB 구축 사업
### 📆_기간

- 2021년 6월 ~ 2021년 7월(2개월)

### 🖍_주제 및 동기

- 덕소 지역의 강수량에 따른 지하수위 변동을 감시하기 위함이다.

### 📃_프로젝트 요약

- 덕소 지역의 8개 관측공에서 측정된 2018년말 부터 2021년 중순까지의 데이터를 이용, 매일 익일부터 7일간의 데이터를 예측한다.
- 예측된 데이터는 실제 데이터와 비교하여 덕소 지역의 지하수 수위를 감시하고 현황을 파악한다.

### 🎭_주요 역할

- 8개 지역 지하수 관측공의 과거 데이터를 전처리하고 시각화 한다.
- LSTM 알고리즘을 이용해 지하수위 예측 모델을 만들고, 일 단위로 지하수위를 감시할 수 있도록 한다.

### 🌞_세부 역할

- **데이터**
    - 학습 및 예측에 사용되는 데이터는 서버에 구축된 PostgreDB에서 psycopg2 패키지를 이용하여 받아왔다.
    - 관측공들은 10분 단위 또는 1시간 단위로 측정되어 있으며 이상치를 비롯하여 일부 기간 결측이 존재했다.
        
        : 롤링함수의 역할을 하는 pandas의 daterange를 이용하여 시간을 index로 지정, 결측 구간을 찾아내고 보간하였다. 그러나 장기간의 결측 구간은 보간하지 말라는 요청이 있었기에 보간하지 않았다.
        
        : 다른 시간 단위는 일 단위를 기준으로 10분 단위 데이터를 다운 샘플링하여 기준을 맞춰주었다.
        
        : 이상치는 EDA를 통해 확인 후, 제거하였고 선형보간하였다.
        
- **모델 구축**
    - 학습된 모델은 60개 셀의 LSTM 레이어 한 층, 50%의 DropOut 한 층, 1개 셀의 Dense 한 층으로 구성되어 있다.
    - 한 타임스텝마다 다음 날의 수위값을 출력하는 형태를 띠고 있으며, 셀의 수와 레이어의 수는 반복 학습을 통한 튜닝을 통해 이루어졌다.
    - 사용된 손실함수는 MSE, 옵티마이저는 Adam, 활성화 함수는 Tanh이다. 손실함수와 옵티마이저는 반복 학습을 통한 튜닝을 통해 선택되었고, Tanh은 LSTM의 특성에 의해 선택되었다.
- **실서비스의 적용**
    - 출력 데이터는 서버 컴퓨터에서 할당된 DB 테이블에 업로드 되었고, sh 파일과 윈도우 스케쥴러를 통해 자동화하였다.
    - DB에서 원본 데이터를 받아 전처리하는 모든 과정과 모델 학습, 출력값 업로드 모두 한 파일로 이루어진 코드로 작성되었다. 모델과 스케일러는 joblib으로 저장하였고, 결과값들은 csv파일로 오프라인 서버컴퓨터에 저장되었다. 동 데이터는 DB에도 업로드 되었다.
    - 지하수의 일 변화를 매일 확인할 수 있도록 sh 파일로 python 언어 파일 지정, 소스 파일을 지정하였고 작동될 수 있게 하였다.
    

### 🛠_사용 기술

- 사용 언어 - Python
- Tool - Jupyter Notebook, VSCode, Google Colab, PostgreSQL
- 사용 라이브러리 - Tensorflow2, Anaconda, Pandas, Numpy
- OS - Window

### 🚣‍♀️_팀 구성

- 사내 총 인원 : 2명
    - DB 관리 : 1명
    - 데이터 분석 : 1명
    

### ‼_이슈

- 데이터 전처리 과정에서 장기간의 결측 구간이 발견되었는데, 해당 기간을 제외하고 가장 긴 기간으로 모델을 만들어달라는 요청이 있었다.
    - 일일이 확인해서 모델 구간을 지정하면 자동화가 불가능하였다. 그래서 null값이 기록된 구간의 기간과 그렇지 않은 구간의 기간을 계산하였고, not null 구간 중 가장 긴 기간을 학습에 사용하였다.
- 받은 데이터의 날짜 모두가 str로 인식되고, pandas의 변환 기능 사용시 1970년으로 인식되는 에러가 있었다.
    - str 날짜를 글자 수대로 슬라이싱하여 각각 ‘년‘, ‘월‘, ‘일‘로 지정하였고 나중에 합치는 과정을 통해 맞는 날짜로 변환하였다.
    

### 💡_배운 것

- DB로부터 데이터를 받아오고 가공된 데이터를 업로드 하는 일련의 과정을 처음 했던 프로젝트였다. 이를 통해 추후 진행된 프로젝트에서 다양한 DB와의 연동을 손쉽게 할 수 있었다.
- 시계열 데이터 가공의 여러 접근법을 연구해 볼 수 있었다. 또한, 결측기간과 기록기간을 set_index를 통해 구분하고 보간하는 방법들에 대해 알게 되었다. 결과적으로 시계열 데이터 가공에 자신감을 가질 수 있었다.
- 모델 뿐만 아니라 스케일러도 저장 후 같은 스케일을 적용하고, transform, inverse_transform의 활용할 수 있다는 것을 배울 수 있었다.
- 작성한 코드와 생성한 모델 등을 Sh 파일을 이용해 설정된 과정을 거치는 자동화 작업을 하고, 나아가 실서비스를 해볼 수 있었던 기회였다.
- 여러 하이퍼파라미터를 반복 학습시키며 session을 초기화하는 법을 알게 되었다.
